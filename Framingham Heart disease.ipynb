{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1d7er7DY-hxdLDsUDTkOFoYX4xuK5gPP5","timestamp":1713465795391}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_2bVwtvjmY0v"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from tabulate import tabulate\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler,LabelEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","import subprocess\n","import logging\n","logging.getLogger('lightgbm').setLevel(logging.ERROR)"]},{"cell_type":"code","source":["def Impute(df):\n","  # Imputation of Missing Values\n","  df[['education','BPMeds','cigsPerDay']] = SimpleImputer(missing_values=np.nan,strategy='most_frequent').fit_transform(df[['education','BPMeds','cigsPerDay']])\n","  df[['totChol','BMI','glucose']] = SimpleImputer(missing_values=np.nan,strategy='mean').fit_transform(df[['totChol','BMI','glucose']])\n","  df[['totChol','glucose']] = df[['totChol','glucose']].astype(int)\n","  df.loc[df.query('heartRate.isna()').index[0],'heartRate'] = np.mean(df['heartRate']).astype(int)\n","  return df"],"metadata":{"id":"lDpMCYUxng07"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def handle_outliers(df):\n","  continuous_cols = ['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n","  for column in continuous_cols:\n","    Q1 = df[column].quantile(0.25)\n","    Q3 = df[column].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n","  return df"],"metadata":{"id":"YnrYLjCNxAJk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def feature(df):\n","  # Feature Engineering\n","  df.insert(df.columns.get_loc('sysBP'),'BP',df['sysBP']+df['diaBP'])\n","  df.insert(df.columns.get_loc('currentSmoker'),'Cigarettes',df['currentSmoker']*df['cigsPerDay'])\n","  df.drop(columns=['sysBP','diaBP'],inplace = True)\n","  df.drop(columns=['currentSmoker','cigsPerDay'],inplace = True)\n","  return df"],"metadata":{"id":"pQh-Am-t5IG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cor(df):\n","  cor=df.iloc[:,:-1].corr()\n","  plt.figure(figsize=(12,12))\n","  sns.heatmap(cor,annot=True,cmap='coolwarm',center=0)\n","  plt.title(\"Correlation Heatmap\")\n","  plt.show()"],"metadata":{"id":"1p-rb9sP2cGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Sample(data):\n","  from sklearn.utils import resample\n","  target_column = 'TenYearCHD'\n","  data_majority = data[data[target_column] == 0]\n","  data_minority = data[data[target_column] == 1]\n","  data_minority_upsampled = resample(data_minority, replace=True, n_samples=int(len(data_majority)*.5), random_state=123)\n","  data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n","  return data_upsampled"],"metadata":{"id":"FJ49F9QW09qD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Scale(df):\n","  X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df.iloc[:,-1], test_size=0.2)\n","  scaler = StandardScaler()\n","  X_train = scaler.fit_transform(X_train)\n","  X_test = scaler.transform(X_test)\n","  return X_train, X_test, y_train, y_test"],"metadata":{"id":"0f47YKZRs_vX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def NoScale(df):\n","  X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df.iloc[:,-1], test_size=0.2)\n","  return X_train, X_test, y_train, y_test"],"metadata":{"id":"xyHcLIu_2OFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","def print_confusion_matrix(y_test, y_pred):\n","\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    num_classes = len(cm)\n","\n","    # Print confusion matrix header\n","    print(\"\\nConfusion Matrix:\")\n","    print(\"True Labels ->\")\n","    print(\"Predicted Labels v\")\n","    print(f\"{'':<10}\", end=\"\")\n","    for i in range(num_classes):\n","        print(f\"{i:<10}\", end=\"\")\n","    print()\n","\n","    # Print confusion matrix contents\n","    for i in range(num_classes):\n","        print(f\"{i:<10}\", end=\"\")\n","        for j in range(num_classes):\n","            print(f\"{cm[i][j]:<10}\", end=\"\")\n","        print()"],"metadata":{"id":"hw_DAbaIdzQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lazyclassify(X_train, X_test, y_train, y_test):\n","\n","  subprocess.check_call([\"pip\", \"install\", \"lazypredict\"])\n","  from lazypredict.Supervised import LazyClassifier\n","\n","  clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n","  models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n","  return models,predictions\n","\n","# print(predictions.iloc[0,:].name)\n","\n","  print(tabulate(models,headers=['Model Name','Accuracy','Balanced Accuracy','ROC AUC','F1 Score','Time Taken'],tablefmt='simple_grid'))"],"metadata":{"id":"4eg6lpP7U4Ex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def LDA(X_train, X_test, y_train, y_test):\n","  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","  lda_model = LinearDiscriminantAnalysis()\n","  lda_model.fit(X_train, y_train)\n","\n","  y_pred = lda_model.predict(X_test)\n","  y_prob = lda_model.predict_proba(X_test)[:,1]\n","\n","  print_confusion_matrix(y_test, y_pred)\n","  Metrics(y_test,y_pred,y_prob)"],"metadata":{"id":"YjsTU8WTmxG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def XGB(X_train, X_test, y_train, y_test):\n","  clf = xgb.XGBClassifier(\n","      objective='binary:logistic',  # for binary classification\n","      n_estimators=100,  # number of trees (boosting rounds)\n","      learning_rate=0.2, # step size shrinkage to prevent overfitting\n","      probability=True\n","      # random_state=42\n","  )\n","\n","  clf.fit(X_train, y_train)\n","  y_pred = clf.predict(X_test)\n","  y_prob = clf.predict_proba(X_test)[:,1]\n","\n","  print_confusion_matrix(y_test, y_pred)\n","  Metrics(y_test,y_pred ,y_prob)"],"metadata":{"id":"NWS1CUmqy1rS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ExtraTree(X_train, X_test, y_train, y_test):\n","  from sklearn.ensemble import ExtraTreesClassifier\n","  clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n","\n","  clf.fit(X_train, y_train)\n","\n","  y_pred = clf.predict(X_test)\n","  y_prob = clf.predict_proba(X_test)[:,1]\n","\n","  print_confusion_matrix(y_test, y_pred)\n","  Metrics(y_test,y_pred ,y_prob)"],"metadata":{"id":"j_SrIpAY6e5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","\n","def Metrics(y_test, y_pred, y_pred_proba):\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    auc = roc_auc_score(y_test, y_pred_proba)\n","\n","    # Create dictionary to store metrics\n","    metrics_dict = {\n","        'Accuracy': round(accuracy*100, 4),\n","        'Precision': round(precision*100, 4),\n","        'Recall': round(recall*100, 4),\n","        'F1': round(f1*100, 4),\n","        'AUC': round(auc*100, 4)\n","    }\n","\n","    # Print the metrics dictionary neatly\n","    print(\"\\nClassification Metrics:\")\n","    for metric, value in metrics_dict.items():\n","        print(f\"{metric} Score: [{value}] %\")\n","\n","    return metrics_dict"],"metadata":{"id":"1NKK0UWVbOQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preProcc(df):\n","  df = Impute(df)\n","  df = handle_outliers(df)\n","  df = Sample(df)\n","  df = feature(df)\n","  # cor(df)\n","  return NoScale(df)\n","\n","def preProccScale(df):\n","  df = Impute(df)\n","  df = handle_outliers(df)\n","  df = Sample(df)\n","  df = feature(df)\n","  # cor(df)\n","  return Scale(df)"],"metadata":{"id":"C5_PSfKj9WT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Predict(X_train, X_test, y_train, y_test):\n","  print(\"\\nFor Lazy Predict Classifier: -\")\n","  lazyclassify(X_train, X_test, y_train, y_test)\n","  print(\"\\nFor XGB Classifier: -\")\n","  XGB(X_train, X_test, y_train, y_test)\n","  print(\"\\nFor LDA Classifier: -\")\n","  LDA(X_train, X_test, y_train, y_test)\n","  # print(\"\\nFor ANN Classification: -\")\n","  # ANN(X_train, X_test, y_train, y_test)\n","  print(\"\\nFor ExtraTrees Classification: -\")\n","  ExtraTree(X_train, X_test, y_train, y_test)"],"metadata":{"id":"z4-oMlW-b1At"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('framingham.csv')\n","print(\"No Scaling of Data: -\\n\")\n","X_train, X_test, y_train, y_test = preProcc(df.copy())\n","Predict(X_train, X_test, y_train, y_test)\n","print(\"\\nWith Scaling of Data: -\")\n","X_train, X_test, y_train, y_test = preProccScale(df)\n","Predict(X_train, X_test, y_train, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mLwy2B2cpiR","executionInfo":{"status":"ok","timestamp":1712518942818,"user_tz":-330,"elapsed":34087,"user":{"displayName":"Jayam Gupta","userId":"06359702269755699757"}},"outputId":"87422f90-264b-4876-a6fe-58a88138171d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["No Scaling of Data: -\n","\n","\n","For Lazy Predict Classifier: -\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29/29 [00:09<00:00,  2.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1416, number of negative: 2899\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000360 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 886\n","[LightGBM] [Info] Number of data points in the train set: 4315, number of used features: 13\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.328158 -> initscore=-0.716530\n","[LightGBM] [Info] Start training from score -0.716530\n","\n","For XGB Classifier: -\n","\n","Confusion Matrix:\n","True Labels ->\n","Predicted Labels v\n","          0         1         \n","0         634       63        \n","1         63        319       \n","\n","Classification Metrics:\n","Accuracy Score: [88.3225] %\n","Precision Score: [83.5079] %\n","Recall Score: [83.5079] %\n","F1 Score: [83.5079] %\n","AUC Score: [92.3704] %\n","\n","For LDA Classifier: -\n","\n","Confusion Matrix:\n","True Labels ->\n","Predicted Labels v\n","          0         1         \n","0         587       110       \n","1         249       133       \n","\n","Classification Metrics:\n","Accuracy Score: [66.7285] %\n","Precision Score: [54.7325] %\n","Recall Score: [34.8168] %\n","F1 Score: [42.56] %\n","AUC Score: [71.632] %\n","\n","For ExtraTrees Classification: -\n","\n","Confusion Matrix:\n","True Labels ->\n","Predicted Labels v\n","          0         1         \n","0         685       12        \n","1         42        340       \n","\n","Classification Metrics:\n","Accuracy Score: [94.9954] %\n","Precision Score: [96.5909] %\n","Recall Score: [89.0052] %\n","F1 Score: [92.6431] %\n","AUC Score: [96.0543] %\n","\n","With Scaling of Data: -\n","\n","For Lazy Predict Classifier: -\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29/29 [00:09<00:00,  2.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 1433, number of negative: 2882\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000374 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 887\n","[LightGBM] [Info] Number of data points in the train set: 4315, number of used features: 13\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.332097 -> initscore=-0.698714\n","[LightGBM] [Info] Start training from score -0.698714\n","\n","For XGB Classifier: -\n","\n","Confusion Matrix:\n","True Labels ->\n","Predicted Labels v\n","          0         1         \n","0         662       52        \n","1         53        312       \n","\n","Classification Metrics:\n","Accuracy Score: [90.2688] %\n","Precision Score: [85.7143] %\n","Recall Score: [85.4795] %\n","F1 Score: [85.5967] %\n","AUC Score: [95.0443] %\n","\n","For LDA Classifier: -\n","\n","Confusion Matrix:\n","True Labels ->\n","Predicted Labels v\n","          0         1         \n","0         624       90        \n","1         224       141       \n","\n","Classification Metrics:\n","Accuracy Score: [70.899] %\n","Precision Score: [61.039] %\n","Recall Score: [38.6301] %\n","F1 Score: [47.3154] %\n","AUC Score: [74.9921] %\n","\n","For ExtraTrees Classification: -\n","\n","Confusion Matrix:\n","True Labels ->\n","Predicted Labels v\n","          0         1         \n","0         702       12        \n","1         26        339       \n","\n","Classification Metrics:\n","Accuracy Score: [96.4782] %\n","Precision Score: [96.5812] %\n","Recall Score: [92.8767] %\n","F1 Score: [94.6927] %\n","AUC Score: [97.6438] %\n"]}]}]}